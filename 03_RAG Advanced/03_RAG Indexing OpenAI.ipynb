{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4658481a-f484-4d44-a742-52504306561a",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG) and Advanced Indexing Techniques**\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant documents to improve their responses. In this note, we explore indexing techniques, multi-representation indexing, and ColBERT-based retrieval. \n",
    "\n",
    "### **Indexing and Multi-Representation Indexing**\n",
    "Indexing is the process of structuring data for efficient retrieval. Traditional vector stores index documents using embeddings, while **multi-representation indexing** improves retrieval by storing multiple vector representations of a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa888bd9-5ad3-40b4-aa88-375d46c13046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "\n",
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "\n",
    "# Required API keys with validation\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b674c15",
   "metadata": {},
   "source": [
    "\n",
    "#### **Implementing Multi-Representation Indexing**\n",
    "#### **Step 1: Load and Process Documents**\n",
    "We use `WebBaseLoader` from `langchain_community` to fetch web content, then split the text using `RecursiveCharacterTextSplitter`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf368e7-ebf6-4469-bfa7-62466184afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d50966",
   "metadata": {},
   "source": [
    "#### **Step 2: Generate Summaries for Efficient Indexing**\n",
    "We summarize each document before indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20622bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\",max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ba780",
   "metadata": {},
   "source": [
    "#### **Step 3: Store Summaries and Documents in a Vector Store**\n",
    "We use **ChromaDB** to store document summaries as embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ad734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187792c6",
   "metadata": {},
   "source": [
    "#### **Step 4: Querying the Indexed Data**\n",
    "To retrieve documents related to a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Memory in agents\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query, n_results=1)\n",
    "retrieved_docs[0].page_content[:500] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e17c89",
   "metadata": {},
   "source": [
    "## **RAPTOR: Advanced Retrieval for RAG**\n",
    "[RAPTOR](https://arxiv.org/pdf/2401.18059.pdf) improves retrieval by dynamically selecting relevant document segments. Instead of treating documents as atomic units, RAPTOR structures and retrieves granular document segments for better LLM performance.\n",
    "\n",
    "- **Key Idea**: Instead of retrieving whole documents, it retrieves relevant sections.\n",
    "- **Implementation**: The method uses **structured retrieval pipelines** with a mix of **vector search, metadata filtering, and reinforcement learning**.\n",
    "\n",
    "> RAPTOR helps RAG systems balance **retrieval efficiency** and **context relevance**.\n",
    "\n",
    "---\n",
    "\n",
    "## **ColBERT: Contextualized Embedding-Based Retrieval**\n",
    "### **What is ColBERT?**\n",
    "[ColBERT (Contextualized Late Interaction over BERT)](https://hackernoon.com/how-colbert-helps-developers-overcome-the-limits-of-rag) refines retrieval by representing each token in a passage as a separate embedding instead of a single vector for the document. \n",
    "\n",
    "- **Unlike standard embeddings**, ColBERT generates multiple context-aware embeddings for each document.\n",
    "- **Late Interaction Mechanism**: Instead of compressing all embeddings into one vector, ColBERT **matches each query token to the most relevant passage token**, improving accuracy.\n",
    "\n",
    "### **Implementing ColBERT in RAG**\n",
    "We use **RAGatouille**, a simplified interface for ColBERT retrieval.\n",
    "\n",
    "#### **Step 1: Install and Load ColBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c9506-c6c0-463b-af77-9291a63f1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79676de8-c93c-415a-bd1a-ba64065bae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71856c5",
   "metadata": {},
   "source": [
    "#### **Step 2: Index a Wikipedia Document**\n",
    "We fetch and index a Wikipedia page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c922b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e56cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b75f00",
   "metadata": {},
   "source": [
    "#### **Step 3: Perform a Semantic Search with ColBERT**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2ad25",
   "metadata": {},
   "source": [
    "#### **Step 4: Use ColBERT as a LangChain Retriever**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797db228",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d8c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
