{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312caf3b",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG) & Advanced Retrieval Techniques**\n",
    "\n",
    "## **1. Introduction to RAG**\n",
    "### **What is Retrieval-Augmented Generation (RAG)?**\n",
    "- RAG combines information retrieval and language generation to improve response accuracy.  \n",
    "- It retrieves relevant documents from a knowledge base before generating a response.  \n",
    "- Used for **Question Answering, Summarization, and Chatbots.**  \n",
    "\n",
    "### **How RAG Works**\n",
    "1. **Query Encoding:** Converts user queries into vector representations.  \n",
    "2. **Document Retrieval:** Searches a vector database for relevant information.  \n",
    "3. **Response Generation:** Uses a language model to generate an answer based on retrieved context.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c4d4d",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Retrieval in RAG**\n",
    "### **Indexing Documents**\n",
    "- Documents are split into **chunks** for efficient retrieval.  \n",
    "- Indexed using **vector databases** (e.g., **Chroma, FAISS, Weaviate**).  \n",
    "- Each chunk is converted into **vector embeddings** using models like **OpenAI Embeddings** or **Cohere Embeddings**.  \n",
    "\n",
    "### **Retrieving Relevant Context**\n",
    "- Uses **similarity search** to find documents relevant to a query.  \n",
    "- Retrieval models:\n",
    "  - **Dense retrieval** (e.g., FAISS, ChromaDB)\n",
    "  - **Sparse retrieval** (e.g., BM25)\n",
    "  - **Hybrid retrieval** (combining dense and sparse methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9393a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "\n",
    "# Required API keys with validation\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fcaf1",
   "metadata": {},
   "source": [
    "\n",
    "## **3. Re-Ranking in RAG**\n",
    "### **What is Re-Ranking?**\n",
    "- Improves the ranking of retrieved documents based on relevance.  \n",
    "- Helps when **retrieved documents are not in the best order** for generation.  \n",
    "\n",
    "### **Re-Ranking Approaches**\n",
    "#### 1. **Reciprocal Rank Fusion (RRF)**\n",
    "   - Combines multiple ranked lists into one.  \n",
    "   - Assigns higher scores to documents appearing in multiple lists.  \n",
    "\n",
    "\n",
    "\n",
    "   **Implementation:**\n",
    "\n",
    "> ##### **We start by preparing the retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4664e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    # embedding=CohereEmbeddings()\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e28a46",
   "metadata": {},
   "source": [
    "### **Create Chat Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG-Fusion\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ac631",
   "metadata": {},
   "source": [
    "### **Reciprocal Rank Fusion (RRF) Implemetaion for Re-Ranking in RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919311be",
   "metadata": {},
   "source": [
    "### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970885f",
   "metadata": {},
   "source": [
    "#### 2. **Cohere Re-Rank**\n",
    "   - Uses a **pre-trained ranking model** from Cohere.  \n",
    "   - Scores retrieved documents and sorts them by relevance.  \n",
    "\n",
    "**Formula:**  \n",
    "\n",
    "$$\n",
    "Score = \\sum \\frac{1}{\\text{Rank} + k}\n",
    "$$\n",
    "\n",
    "(where **k** is a smoothing factor)\n",
    "\n",
    "\n",
    "   **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eba70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain.retrievers import  ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553eb87",
   "metadata": {},
   "source": [
    "## **4. CRAG (Context-Rich Augmented Generation)**\n",
    "### **What is CRAG?**\n",
    "- Enhances RAG by improving **retrieval quality and context selection.**  \n",
    "- Ensures that retrieved documents **contain rich, high-value information** instead of noise.  \n",
    "- Uses **filtering and ranking techniques** to optimize context.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d030b96",
   "metadata": {},
   "source": [
    "### **CRAG vs Standard RAG**\n",
    "| Feature       | Standard RAG | CRAG |\n",
    "|--------------|-------------|------|\n",
    "| Simple Retrieval | ✅ | ✅ |\n",
    "| Context Filtering | ❌ | ✅ |\n",
    "| Re-Ranking | ❌ | ✅ |\n",
    "| Knowledge Optimization | ❌ | ✅ |\n",
    "\n",
    "### **Resources for CRAG**\n",
    "- 📺 **Deep Dive Video:** [YouTube](https://www.youtube.com/watch?v=E2shqsYwxck)  \n",
    "- 📄 **Notebooks:**\n",
    "  - [LangGraph CRAG](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb)  \n",
    "  - [LangGraph CRAG Mistral](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_mistral.ipynb)  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Self-RAG**\n",
    "### **What is Self-RAG?**\n",
    "- A **self-improving RAG model** that **learns to retrieve better over time.**  \n",
    "- Uses **feedback loops** to refine retrieval and context selection.  \n",
    "\n",
    "### **How Self-RAG Works**\n",
    "1. **Initial Retrieval:** Retrieve documents for a query.  \n",
    "2. **Generation & Feedback:** Generate a response and assess quality.  \n",
    "3. **Self-Improvement:** Modify retrieval behavior based on feedback.  \n",
    "\n",
    "### **Resources for Self-RAG**\n",
    "- 📄 **Notebooks:**  \n",
    "  - [Self-RAG Example](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_mistral_nomic.ipynb)  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Impact of Long Context on Retrieval**\n",
    "### **Challenges with Long Context**\n",
    "- Large context windows **increase computation cost.**  \n",
    "- **Irrelevant information** can dilute useful context.  \n",
    "- **Token limitations** in LLMs can affect response quality.  \n",
    "\n",
    "### **Solutions**\n",
    "1. **Efficient Chunking:**  \n",
    "   - Use **RecursiveCharacterTextSplitter** to split text intelligently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710296b",
   "metadata": {},
   "source": [
    "2. **Adaptive Context Selection:**  \n",
    "   - Rank retrieved documents based on **semantic similarity** rather than just keyword matching.  \n",
    "\n",
    "### **Resources on Long Context**\n",
    "- 📺 **Deep Dive Video:** [YouTube](https://www.youtube.com/watch?v=SsHUNfhF32s)  \n",
    "- 📑 **Slides:** [Google Slides](https://docs.google.com/presentation/d/1mJUiPBdtf58NfuSEQ7pVSEQ2Oqmek7F1i4gBwR6JDss/edit#slide=id.g26c0cb8dc66_0_0)  \n",
    "\n",
    "---\n",
    "\n",
    "# **Conclusion**\n",
    "### **Key Takeaways**\n",
    "- **RAG** enhances language models by incorporating external knowledge.  \n",
    "- **Re-Ranking** improves retrieved document quality.  \n",
    "- **CRAG** optimizes retrieval by filtering low-value context.  \n",
    "- **Self-RAG** enables models to learn and improve retrieval over time.  \n",
    "- **Long Context** can degrade retrieval efficiency but can be managed using **chunking and ranking techniques.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Further Reading & References**\n",
    "-  **LangChain RAG Guide:** [LangChain Docs](https://python.langchain.com/docs/use_cases/question_answering)  \n",
    "-  **LangGraph RAG Examples:** [GitHub](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)  \n",
    "-  **YouTube Lectures:**  \n",
    "  - [Self-RAG Deep Dive](https://www.youtube.com/watch?v=E2shqsYwxck)  \n",
    "  - [Impact of Long Context](https://www.youtube.com/watch?v=SsHUNfhF32s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e80b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
