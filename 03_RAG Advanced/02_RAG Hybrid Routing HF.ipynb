{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing Hybrid Routing (Embedding-Based + Logical & Semantic Routing)**\n",
    "We'll implement a **hybrid routing system** that first uses **embedding-based similarity search** to find the most relevant knowledge source, and then applies **logical & semantic routing** using an LLM to refine the selection.\n",
    "\n",
    "\n",
    "### **Install Required Libraries**\n",
    "Ensure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai langchain-huggingface faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Your Knowledge Sources**\n",
    "We assume we have three document sources:  \n",
    "- **Python Docs**  \n",
    "- **JavaScript Docs**  \n",
    "- **Golang Docs**  \n",
    "\n",
    "Each source contains **precomputed embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector databases for each knowledge source\n",
    "vector_stores = {\n",
    "    \"python_docs\": FAISS.load_local(\"faiss_index/python\", embedding_model),\n",
    "    \"js_docs\": FAISS.load_local(\"faiss_index/javascript\", embedding_model),\n",
    "    \"golang_docs\": FAISS.load_local(\"faiss_index/golang\", embedding_model),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Embedding-Based Routing**\n",
    "This step **converts the query into an embedding** and finds the **closest** knowledge source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_based_routing(query: str):\n",
    "    \"\"\"Find the most relevant knowledge source based on embedding similarity.\"\"\"\n",
    "    max_score = -1\n",
    "    best_source = None\n",
    "\n",
    "    # Compute query embedding\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "    for source, db in vector_stores.items():\n",
    "        docs = db.similarity_search_by_vector(query_embedding, k=1)  # Retrieve top 1 doc\n",
    "        score = docs[0].metadata.get(\"score\", 0)  # Get similarity score\n",
    "\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_source = source\n",
    "\n",
    "    return best_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What the above code does:**  \n",
    "- Converts the **query** into an embedding  \n",
    "- Finds the **most similar document** from each **vector database**  \n",
    "- Picks the **highest-scoring** source  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Logical & Semantic Routing with an LLM**\n",
    "Once we get the **best source**, we use an **LLM** to **validate the selection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define routing model output\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM (Mistral-7B or OpenAI)\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    temperature=0.1,\n",
    "    max_length=512\n",
    ")\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM routing prompt\n",
    "system_prompt = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, choose the best source:\n",
    "- \"python_docs\" for Python-related questions.\n",
    "- \"js_docs\" for JavaScript-related questions.\n",
    "- \"golang_docs\" for Golang-related questions.\n",
    "\n",
    "If unsure, choose the closest match.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Define router\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What This Does:**  \n",
    "- Uses an **LLM** to **refine** the routing decision  \n",
    "- If embedding-based routing picks **Python**, but the LLM thinks **JavaScript**, it adjusts  \n",
    "- Ensures **better accuracy** than embeddings alone  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Hybrid Routing Logic**\n",
    "Now, we **combine both methods** for optimal routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_routing(query: str):\n",
    "    \"\"\"Combines embedding-based and LLM-based routing.\"\"\"\n",
    "    \n",
    "    # Step 1: Use embedding-based routing\n",
    "    embedding_source = embedding_based_routing(query)\n",
    "    print(f\"üîç Embedding-based suggestion: {embedding_source}\")\n",
    "\n",
    "    # Step 2: Validate with LLM-based routing\n",
    "    llm_source = router.invoke({\"question\": query}).datasource\n",
    "    print(f\"ü§ñ LLM-based suggestion: {llm_source}\")\n",
    "\n",
    "    # Step 3: Final decision (prefer LLM if different)\n",
    "    final_source = llm_source if llm_source != embedding_source else embedding_source\n",
    "    print(f\"‚úÖ Final decision: {final_source}\")\n",
    "\n",
    "    return final_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Retrieve & Generate Answer**\n",
    "Once we **route** the query, we retrieve **relevant documents** and use an **LLM** to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_answer(query: str):\n",
    "    \"\"\"Retrieve relevant documents and generate an answer.\"\"\"\n",
    "    \n",
    "    # Step 1: Determine knowledge source\n",
    "    best_source = hybrid_routing(query)\n",
    "\n",
    "    # Step 2: Retrieve documents\n",
    "    docs = vector_stores[best_source].similarity_search(query, k=3)\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Answer the question based on this context:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "    )\n",
    "    answer_chain = answer_prompt | llm\n",
    "\n",
    "    answer = answer_chain.invoke({\"context\": context, \"question\": query})\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test the System**\n",
    "Now, let's **test** the hybrid routing and answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I create a dictionary in Python?\"\n",
    "response = retrieve_and_answer(query)\n",
    "\n",
    "print(\"\\nüí° Final Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of What We Built**\n",
    "1Ô∏è **Embedding-Based Routing**  \n",
    "   - Uses **vector similarity** to find the best **knowledge source**.  \n",
    "\n",
    "2Ô∏è **Logical & Semantic Routing**  \n",
    "   - Uses an **LLM** to **validate and refine** the decision.  \n",
    "\n",
    "3Ô∏è **Hybrid Routing**   \n",
    "   - Uses **both** methods for more **accurate** and **robust** retrieval.  \n",
    "\n",
    "4Ô∏è **Final Answer Generation**  \n",
    "   - Retrieves **relevant documents** and generates an answer using an LLM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
